{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7517ed2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GROOVE: A Generative Refinement Framework Using Vision-Language Feedback Loops\n",
    "#\n",
    "# This script implements the core logic for the GROOVE agentic system. It uses\n",
    "# a multi-agent workflow to autonomously refine text-to-image prompts to match\n",
    "# a target image.\n",
    "#\n",
    "# AGENTS:\n",
    "# 1. Descriptor (BLIP): Provides an initial text description of a target image.\n",
    "# 2. Refiner (Gemini): Intelligently refines the prompts based on performance.\n",
    "# 3. Generator (SDXL): Creates an image from the prompts.\n",
    "# 4. Comparator (CLIP): Scores the similarity between the generated and target images.\n",
    "#\n",
    "# USAGE:\n",
    "# 1. Ensure all dependencies in requirements.txt are installed.\n",
    "# 2. Configure your Gemini API key in Google Colab Secrets.\n",
    "# 3. Place a target image named 'my_image.jpg' in the root content directory.\n",
    "# 4. Run the script.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9461a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers diffusers torch accelerate scipy Pillow tqdm google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9628b40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# --- Suppress specific warnings for cleaner output ---\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ac3ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration Constants ---\n",
    "INITIAL_IMAGE_PATH = \"/content/my_image.jpg\" # Path to the target image.\n",
    "OUTPUT_DIR = \"output\"                        # Directory to save generated images.\n",
    "MAX_ITERATIONS = 5                           # Maximum number of refinement loops.\n",
    "SIMILARITY_THRESHOLD = 0.95                  # Exit loop if score exceeds this value.\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def configure_llm():\n",
    "    \"\"\"\n",
    "    Configures and initializes the Google Gemini LLM for prompt refinement.\n",
    "    \n",
    "    This function retrieves the API key from Colab secrets and initializes the\n",
    "    generative model.\n",
    "\n",
    "    Returns:\n",
    "        genai.GenerativeModel: An initialized Gemini model instance.\n",
    "        None: If the API key is not found or an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('GEMINI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise KeyError(\"GEMINI_API_KEY not found in Colab secrets.\")\n",
    "        genai.configure(api_key=api_key)\n",
    "        print(\"Gemini API configured successfully.\")\n",
    "        return genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    except (ImportError, KeyError) as e:\n",
    "        print(f\"Error: Could not configure Gemini API. {e}\")\n",
    "        print(\"Please follow the setup instructions in the README to add your API key.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3854c33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_setup():\n",
    "    \"\"\"\n",
    "    Verifies that the environment is set up correctly before starting.\n",
    "    \n",
    "    Checks for the initial image file and creates the output directory if needed.\n",
    "    \"\"\"\n",
    "    print(\"--- System Check ---\")\n",
    "    if not os.path.exists(INITIAL_IMAGE_PATH):\n",
    "        print(f\"Error: Initial image not found at '{INITIAL_IMAGE_PATH}'\")\n",
    "        exit()\n",
    "    print(f\"Initial image found: {INITIAL_IMAGE_PATH}\")\n",
    "\n",
    "    if DEVICE == \"cpu\":\n",
    "        print(\"Warning: No GPU detected. The process will be extremely slow.\")\n",
    "    else:\n",
    "        print(f\"Success: GPU detected. Using device: '{DEVICE}'\")\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Output directory is '{OUTPUT_DIR}'\")\n",
    "    print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae4162",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Loads all required AI models into memory and onto the specified device.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the initialized model instances and processors.\n",
    "    \"\"\"\n",
    "    print(\"Loading models... This may take several minutes.\")\n",
    "    llm_model = configure_llm()\n",
    "    if llm_model is None:\n",
    "        exit() # Exit if API key is not configured\n",
    "\n",
    "    print(\"Loading Descriptor: BLIP...\")\n",
    "    descriptor_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "    descriptor_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(DEVICE)\n",
    "    \n",
    "    print(\"Loading Generator: Stable Diffusion XL...\")\n",
    "    generator_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    print(\"Loading Comparator: CLIP...\")\n",
    "    comparator_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "    comparator_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    print(\"All models loaded.\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"descriptor\": (descriptor_model, descriptor_processor),\n",
    "        \"generator\": generator_pipe,\n",
    "        \"comparator\": (comparator_model, comparator_processor),\n",
    "        \"llm_refiner\": llm_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14749ee0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def describe_image(image: Image.Image, model, processor):\n",
    "    \"\"\"\n",
    "    Generates a base text description (caption) for a given image using BLIP.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The input image.\n",
    "        model: The loaded BLIP model.\n",
    "        processor: The loaded BLIP processor.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text caption for the image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "    output = model.generate(**inputs, max_length=75)\n",
    "    return processor.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdeda11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def refine_prompts_with_llm(llm_model, base_description, previous_prompt, previous_score):\n",
    "    \"\"\"\n",
    "    Uses a text-based LLM to intelligently refine prompts.\n",
    "\n",
    "    It generates both a new positive prompt and a negative prompt based on\n",
    "    the performance of the previous attempt.\n",
    "\n",
    "    Args:\n",
    "        llm_model: The initialized Gemini model instance.\n",
    "        base_description (str): The original, clean description of the target image.\n",
    "        previous_prompt (str): The positive prompt used in the last iteration.\n",
    "        previous_score (float): The similarity score from the last iteration.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing \"positive_prompt\" and \"negative_prompt\".\n",
    "    \"\"\"\n",
    "    if llm_model is None: \n",
    "        return {\"positive_prompt\": previous_prompt, \"negative_prompt\": \"\"}\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are an expert prompt engineer for text-to-image models. Your task is to refine a positive prompt and generate a helpful negative prompt to make the generated image more visually similar to a ground truth description.\n",
    "\n",
    "    **Ground Truth Description:**\n",
    "    \"{base_description}\"\n",
    "\n",
    "    **Previous Attempt:**\n",
    "    - **Prompt Used:** \"{previous_prompt}\"\n",
    "    - **Similarity Score (0.0 to 1.0):** {previous_score:.4f}\n",
    "\n",
    "    **Your Instructions:**\n",
    "    1. Analyze the \"Previous Prompt\" and rewrite a new, improved positive prompt. Incorporate specific visual details from the \"Ground Truth Description\" and add quality keywords (e.g., \"photorealistic, 4k, sharp focus\").\n",
    "    2. Based on the \"Ground Truth Description\", create a helpful negative prompt. List keywords for things to avoid (e.g., if the description is a photo, the negative prompt could be \"cartoon, anime, painting, watermark, text, blurry\").\n",
    "    3. Your output **MUST** be a single, valid JSON object with two keys: \"positive_prompt\" and \"negative_prompt\". Do not include any other text or markdown formatting.\n",
    "\n",
    "    **JSON Output:**\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm_model.generate_content(metaprompt)\n",
    "        # LLMs may wrap their output in markdown fences; this removes them.\n",
    "        json_str = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        prompts = json.loads(json_str)\n",
    "        return prompts\n",
    "    except (json.JSONDecodeError, Exception) as e:\n",
    "        print(f\"Error during LLM refinement: {e}\")\n",
    "        # Fallback to previous prompts if refinement fails\n",
    "        return {\"positive_prompt\": previous_prompt, \"negative_prompt\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9a626",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_image(prompt: str, negative_prompt: str, pipe, generator_seed=42):\n",
    "    \"\"\"\n",
    "    Generates an image using the Stable Diffusion XL pipeline.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The positive prompt describing the desired image.\n",
    "        negative_prompt (str): The negative prompt describing what to avoid.\n",
    "        pipe: The initialized Stable Diffusion XL pipeline.\n",
    "        generator_seed (int): A seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: The generated image.\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=DEVICE).manual_seed(generator_seed)\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        generator=generator,\n",
    "        num_inference_steps=25\n",
    "    ).images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f83a3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compare_images(image1: Image.Image, image2: Image.Image, model, processor):\n",
    "    \"\"\"\n",
    "    Compares two images using CLIP and returns a semantic similarity score.\n",
    "\n",
    "    Args:\n",
    "        image1 (PIL.Image.Image): The first image (typically the original).\n",
    "        image2 (PIL.Image.Image): The second image (typically the generated one).\n",
    "        model: The loaded CLIP model.\n",
    "        processor: The loaded CLIP processor.\n",
    "\n",
    "    Returns:\n",
    "        float: A similarity score between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    inputs = processor(text=None, images=[image1, image2], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # Normalize features to compute cosine similarity\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    similarity = (image_features[0] @ image_features[1]).item()\n",
    "    \n",
    "    # Scale score from [-1, 1] range to [0, 1] for easier interpretation\n",
    "    return (similarity + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212690b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to run the GROOVE agentic loop.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup and Model Loading ---\n",
    "    check_setup()\n",
    "    models = load_models()\n",
    "    \n",
    "    descriptor_model, descriptor_processor = models[\"descriptor\"]\n",
    "    generator_pipe = models[\"generator\"]\n",
    "    comparator_model, comparator_processor = models[\"comparator\"]\n",
    "    llm_refiner = models[\"llm_refiner\"]\n",
    "\n",
    "    # --- 2. Initialization ---\n",
    "    initial_image = Image.open(INITIAL_IMAGE_PATH).convert(\"RGB\")\n",
    "    \n",
    "    print(\"Generating base description for reference...\")\n",
    "    base_image_description = describe_image(initial_image, descriptor_model, descriptor_processor)\n",
    "    print(f\"Base Description: \\\"{base_image_description}\\\"\\n\")\n",
    "\n",
    "    best_positive_prompt = base_image_description\n",
    "    best_negative_prompt = \"blurry, low quality, text, watermark, deformed, ugly, bad anatomy\"\n",
    "    best_image = None\n",
    "    best_score = -1.0\n",
    "    \n",
    "    current_positive_prompt = best_positive_prompt\n",
    "    current_negative_prompt = best_negative_prompt\n",
    "    \n",
    "    # --- 3. Agentic Iteration Loop ---\n",
    "    print(\"--- Starting Agentic Iteration Loop ---\")\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        print(f\"\\n===== Iteration {i + 1}/{MAX_ITERATIONS} =====\")\n",
    "        \n",
    "        # Refine prompts on every iteration after the first one\n",
    "        if i > 0:\n",
    "            print(\"Refining prompts with LLM...\")\n",
    "            refined_prompts = refine_prompts_with_llm(\n",
    "                llm_refiner,\n",
    "                base_description=base_image_description,\n",
    "                previous_prompt=best_positive_prompt,\n",
    "                previous_score=best_score\n",
    "            )\n",
    "            current_positive_prompt = refined_prompts.get(\"positive_prompt\", best_positive_prompt)\n",
    "            current_negative_prompt = refined_prompts.get(\"negative_prompt\", best_negative_prompt)\n",
    "        \n",
    "        # Log current prompts and generate the new image\n",
    "        print(f\"Positive Prompt: \\\"{current_positive_prompt}\\\"\")\n",
    "        print(f\"Negative Prompt: \\\"{current_negative_prompt}\\\"\")\n",
    "        generated_image = generate_image(\n",
    "            prompt=current_positive_prompt,\n",
    "            negative_prompt=current_negative_prompt,\n",
    "            pipe=generator_pipe\n",
    "        )\n",
    "        \n",
    "        # Compare the new image and evaluate the performance\n",
    "        print(\"Comparing images...\")\n",
    "        similarity_score = compare_images(initial_image, generated_image, comparator_model, comparator_processor)\n",
    "        print(f\"Similarity Score: {similarity_score:.4f}\")\n",
    "\n",
    "        # If the score has improved, save the new best results\n",
    "        if similarity_score > best_score:\n",
    "            print(f\"New best score found! Previous best: {best_score:.4f}\")\n",
    "            best_score = similarity_score\n",
    "            best_positive_prompt = current_positive_prompt\n",
    "            best_negative_prompt = current_negative_prompt\n",
    "            best_image = generated_image\n",
    "            \n",
    "            save_path = os.path.join(OUTPUT_DIR, f\"best_image_iteration_{i+1}.png\")\n",
    "            best_image.save(save_path)\n",
    "            print(f\"Saved new best image to '{save_path}'\")\n",
    "\n",
    "        # Check for early exit if a high enough score is achieved\n",
    "        if best_score >= SIMILARITY_THRESHOLD:\n",
    "            print(f\"\\nSimilarity threshold of {SIMILARITY_THRESHOLD} reached. Halting.\")\n",
    "            break\n",
    "\n",
    "    # --- 4. Final Output ---\n",
    "    print(\"\\n--- Agentic Loop Finished ---\")\n",
    "    print(f\"Final Best Score: {best_score:.4f}\")\n",
    "    print(f\"Final Best Positive Prompt: \\\"{best_positive_prompt}\\\"\")\n",
    "    print(f\"Final Best Negative Prompt: \\\"{best_negative_prompt}\\\"\")\n",
    "    \n",
    "    final_image_path = os.path.join(OUTPUT_DIR, \"final_best_image.png\")\n",
    "    if best_image:\n",
    "        best_image.save(final_image_path)\n",
    "        print(f\"The best overall image has been saved to '{final_image_path}'\")\n",
    "\n",
    "# --- Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # This block ensures the main function is called only when the script is executed directly\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b16cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Happy Coding!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
